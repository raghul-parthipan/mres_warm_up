{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Examination and Preparation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('GTEX_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2LD1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAED1</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>tissue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GTEX-1117F</td>\n",
       "      <td>0.738055</td>\n",
       "      <td>-0.177512</td>\n",
       "      <td>-2.565279</td>\n",
       "      <td>1.182233</td>\n",
       "      <td>-0.090569</td>\n",
       "      <td>-2.314897</td>\n",
       "      <td>-0.468852</td>\n",
       "      <td>0.760869</td>\n",
       "      <td>0.270284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572460</td>\n",
       "      <td>-2.077009</td>\n",
       "      <td>-0.081923</td>\n",
       "      <td>-1.613992</td>\n",
       "      <td>-0.618748</td>\n",
       "      <td>-1.798076</td>\n",
       "      <td>2.565279</td>\n",
       "      <td>2.565279</td>\n",
       "      <td>-2.204469</td>\n",
       "      <td>Adipose_Subcutaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GTEX-111CU</td>\n",
       "      <td>0.623972</td>\n",
       "      <td>0.212631</td>\n",
       "      <td>-0.699026</td>\n",
       "      <td>0.328851</td>\n",
       "      <td>0.623972</td>\n",
       "      <td>0.217038</td>\n",
       "      <td>0.743722</td>\n",
       "      <td>0.337958</td>\n",
       "      <td>0.704535</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522425</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>-0.960567</td>\n",
       "      <td>1.053534</td>\n",
       "      <td>-0.221450</td>\n",
       "      <td>-1.016737</td>\n",
       "      <td>0.760869</td>\n",
       "      <td>1.016737</td>\n",
       "      <td>-0.488170</td>\n",
       "      <td>Adipose_Subcutaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GTEX-111FC</td>\n",
       "      <td>0.517495</td>\n",
       "      <td>-0.552281</td>\n",
       "      <td>0.177512</td>\n",
       "      <td>0.270284</td>\n",
       "      <td>0.507671</td>\n",
       "      <td>0.426007</td>\n",
       "      <td>-1.217622</td>\n",
       "      <td>-0.778242</td>\n",
       "      <td>-1.038647</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303395</td>\n",
       "      <td>0.517495</td>\n",
       "      <td>0.572460</td>\n",
       "      <td>-0.230286</td>\n",
       "      <td>0.557304</td>\n",
       "      <td>0.068965</td>\n",
       "      <td>-0.151318</td>\n",
       "      <td>-1.190940</td>\n",
       "      <td>1.190940</td>\n",
       "      <td>Adipose_Subcutaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GTEX-111VG</td>\n",
       "      <td>1.539381</td>\n",
       "      <td>0.881575</td>\n",
       "      <td>0.738055</td>\n",
       "      <td>1.009542</td>\n",
       "      <td>-0.370057</td>\n",
       "      <td>0.416593</td>\n",
       "      <td>-0.837945</td>\n",
       "      <td>-0.473665</td>\n",
       "      <td>0.454478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567394</td>\n",
       "      <td>1.798076</td>\n",
       "      <td>-1.798076</td>\n",
       "      <td>-2.702943</td>\n",
       "      <td>-0.913795</td>\n",
       "      <td>-1.264124</td>\n",
       "      <td>2.204469</td>\n",
       "      <td>0.974322</td>\n",
       "      <td>-1.680894</td>\n",
       "      <td>Adipose_Subcutaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GTEX-111YS</td>\n",
       "      <td>-1.068659</td>\n",
       "      <td>-0.056019</td>\n",
       "      <td>0.077602</td>\n",
       "      <td>1.131768</td>\n",
       "      <td>-0.507671</td>\n",
       "      <td>-1.053534</td>\n",
       "      <td>0.920353</td>\n",
       "      <td>-0.407217</td>\n",
       "      <td>0.212631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430727</td>\n",
       "      <td>0.274754</td>\n",
       "      <td>-0.693537</td>\n",
       "      <td>-0.047394</td>\n",
       "      <td>1.031289</td>\n",
       "      <td>0.248014</td>\n",
       "      <td>-1.472472</td>\n",
       "      <td>0.981268</td>\n",
       "      <td>-0.203828</td>\n",
       "      <td>Adipose_Subcutaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 12559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      A1BG     A2LD1       A2M     A2ML1    A4GALT      AAAS  \\\n",
       "0  GTEX-1117F  0.738055 -0.177512 -2.565279  1.182233 -0.090569 -2.314897   \n",
       "1  GTEX-111CU  0.623972  0.212631 -0.699026  0.328851  0.623972  0.217038   \n",
       "2  GTEX-111FC  0.517495 -0.552281  0.177512  0.270284  0.507671  0.426007   \n",
       "3  GTEX-111VG  1.539381  0.881575  0.738055  1.009542 -0.370057  0.416593   \n",
       "4  GTEX-111YS -1.068659 -0.056019  0.077602  1.131768 -0.507671 -1.053534   \n",
       "\n",
       "       AACS     AAED1     AAGAB  ...    ZWILCH     ZWINT      ZXDA      ZXDB  \\\n",
       "0 -0.468852  0.760869  0.270284  ...  0.572460 -2.077009 -0.081923 -1.613992   \n",
       "1  0.743722  0.337958  0.704535  ... -0.522425  0.306203 -0.960567  1.053534   \n",
       "2 -1.217622 -0.778242 -1.038647  ...  1.303395  0.517495  0.572460 -0.230286   \n",
       "3 -0.837945 -0.473665  0.454478  ...  0.567394  1.798076 -1.798076 -2.702943   \n",
       "4  0.920353 -0.407217  0.212631  ... -0.430727  0.274754 -0.693537 -0.047394   \n",
       "\n",
       "       ZXDC    ZYG11B       ZYX     ZZEF1      ZZZ3                tissue  \n",
       "0 -0.618748 -1.798076  2.565279  2.565279 -2.204469  Adipose_Subcutaneous  \n",
       "1 -0.221450 -1.016737  0.760869  1.016737 -0.488170  Adipose_Subcutaneous  \n",
       "2  0.557304  0.068965 -0.151318 -1.190940  1.190940  Adipose_Subcutaneous  \n",
       "3 -0.913795 -1.264124  2.204469  0.974322 -1.680894  Adipose_Subcutaneous  \n",
       "4  1.031289  0.248014 -1.472472  0.981268 -0.203828  Adipose_Subcutaneous  \n",
       "\n",
       "[5 rows x 12559 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15201, 12559)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sorted_id = dataset.sort_values(by=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is sorted so that the test and training set won't have the same person's entries in each, in order to prevent data leakage. \n",
    "\n",
    "Note that this method is not perfect and will allow one person's entries to appear in both test and training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sorted_id.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No NANs in dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = dataset.sample(10000)\n",
    "#sample a section of the dataset if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataframe, test_size, random_state=42):\n",
    "    \"\"\"This function splits the dataframe specified into a train set and a test set. The split proportion is determined by\n",
    "    the specified test_size. A stratified split (stratified by tissue) is performed and it is ensured that no patient appears\n",
    "    in both the test and train split (to ensure independence)\n",
    "    \n",
    "    It is very slow to run.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    for train_index, test_index in split.split(dataframe, dataframe[\"tissue\"]):\n",
    "            strat_train_set_full = dataframe.iloc[train_index]\n",
    "            strat_test_set = dataframe.iloc[test_index]\n",
    "            \n",
    "    for patient in dataframe[\"Unnamed: 0\"].unique():\n",
    "        if patient in strat_train_set_full['Unnamed: 0'].values and patient in strat_test_set['Unnamed: 0'].values:\n",
    "            len_train = strat_train_set_full['Unnamed: 0'].value_counts()[patient]\n",
    "            len_test = strat_test_set['Unnamed: 0'].value_counts()[patient]\n",
    "            if len_train > 0 and len_test > 0:\n",
    "                if len_test > len_train:\n",
    "                    rows = strat_train_set_full.loc[lambda strat_train_set_full: strat_train_set_full['Unnamed: 0'] == patient]\n",
    "                    strat_test_set = strat_test_set.append(rows, ignore_index=True)\n",
    "                    strat_train_set_full.drop(rows.index, inplace=True)\n",
    "                else:\n",
    "                    rows = strat_test_set.loc[lambda strat_test_set: strat_test_set['Unnamed: 0'] == patient]\n",
    "                    strat_train_set_full = strat_train_set_full.append(rows, ignore_index=True)\n",
    "                    strat_test_set.drop(rows.index, inplace=True)\n",
    "                    \n",
    "    return strat_train_set_full, strat_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = stratified_split(dataset, 0.3)\n",
    "#slow to run. too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to speed the above up + create validation set + make into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non stratified split method\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = dataset_sorted_id.drop([\"Unnamed: 0\", \"tissue\"], axis=1)\n",
    "target = dataset_sorted_id[\"tissue\"]\n",
    "\n",
    "#label encode targets\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target)\n",
    "\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "#scale all features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2037, 12557)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_classes = len(le.classes_)\n",
    "number_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can fit to training set. Poor result on test set -> overfitting. Can examine with hyperparameter tuning later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "\n",
    "  'n_estimators': sp_randint(200,600),\n",
    "  'max_depth':  sp_randint(20,110),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] n_estimators=548, max_depth=71 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 548 out of 548 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 480 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 548 out of 548 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=548, max_depth=71, score=0.578, total= 2.6min\n",
      "[CV] n_estimators=548, max_depth=71 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   52.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 548 out of 548 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 480 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 548 out of 548 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=548, max_depth=71, score=0.578, total= 2.1min\n",
      "[CV] n_estimators=548, max_depth=71 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 548 out of 548 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 480 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 548 out of 548 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=548, max_depth=71, score=0.578, total= 2.8min\n",
      "[CV] n_estimators=306, max_depth=34 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 306 out of 306 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 306 out of 306 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=306, max_depth=34, score=0.545, total= 1.6min\n",
      "[CV] n_estimators=306, max_depth=34 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 306 out of 306 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 306 out of 306 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=306, max_depth=34, score=0.543, total= 1.5min\n",
      "[CV] n_estimators=306, max_depth=34 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 306 out of 306 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 306 out of 306 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=306, max_depth=34, score=0.530, total= 1.6min\n",
      "[CV] n_estimators=388, max_depth=91 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 388 out of 388 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 388 out of 388 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=388, max_depth=91, score=0.557, total= 1.8min\n",
      "[CV] n_estimators=388, max_depth=91 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 388 out of 388 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 388 out of 388 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=388, max_depth=91, score=0.561, total= 2.1min\n",
      "[CV] n_estimators=388, max_depth=91 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 388 out of 388 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 388 out of 388 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=388, max_depth=91, score=0.562, total= 2.1min\n",
      "[CV] n_estimators=302, max_depth=40 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 302 out of 302 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 302 out of 302 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=302, max_depth=40, score=0.551, total= 1.6min\n",
      "[CV] n_estimators=302, max_depth=40 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 302 out of 302 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 302 out of 302 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=302, max_depth=40, score=0.545, total= 1.5min\n",
      "[CV] n_estimators=302, max_depth=40 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 302 out of 302 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 302 out of 302 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... n_estimators=302, max_depth=40, score=0.541, total= 1.7min\n",
      "[CV] n_estimators=414, max_depth=102 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 414 out of 414 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 414 out of 414 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... n_estimators=414, max_depth=102, score=0.565, total= 2.1min\n",
      "[CV] n_estimators=414, max_depth=102 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 414 out of 414 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 414 out of 414 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... n_estimators=414, max_depth=102, score=0.564, total= 2.0min\n",
      "[CV] n_estimators=414, max_depth=102 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 414 out of 414 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 414 out of 414 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... n_estimators=414, max_depth=102, score=0.560, total= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 29.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 548building tree 2 of 548\n",
      "building tree 3 of 548\n",
      "building tree 4 of 548building tree 5 of 548\n",
      "building tree 6 of 548\n",
      "building tree 7 of 548\n",
      "building tree 8 of 548\n",
      "building tree 9 of 548\n",
      "building tree 10 of 548\n",
      "building tree 11 of 548\n",
      "\n",
      "building tree 12 of 548building tree 13 of 548building tree 14 of 548\n",
      "\n",
      "\n",
      "\n",
      "building tree 15 of 548building tree 16 of 548\n",
      "\n",
      "building tree 17 of 548\n",
      "building tree 18 of 548\n",
      "building tree 19 of 548\n",
      "building tree 20 of 548\n",
      "building tree 21 of 548\n",
      "building tree 22 of 548\n",
      "building tree 23 of 548\n",
      "building tree 24 of 548\n",
      "building tree 25 of 548\n",
      "building tree 26 of 548\n",
      "building tree 27 of 548\n",
      "building tree 28 of 548\n",
      "building tree 29 of 548\n",
      "building tree 30 of 548\n",
      "building tree 31 of 548\n",
      "building tree 32 of 548\n",
      "building tree 33 of 548\n",
      "building tree 34 of 548\n",
      "building tree 35 of 548\n",
      "building tree 36 of 548\n",
      "building tree 37 of 548\n",
      "building tree 38 of 548\n",
      "building tree 39 of 548\n",
      "building tree 40 of 548\n",
      "building tree 41 of 548\n",
      "building tree 42 of 548\n",
      "building tree 43 of 548\n",
      "building tree 44 of 548\n",
      "building tree 45 of 548\n",
      "building tree 46 of 548\n",
      "building tree 47 of 548\n",
      "building tree 48 of 548\n",
      "building tree 49 of 548\n",
      "building tree 50 of 548\n",
      "building tree 51 of 548\n",
      "building tree 52 of 548\n",
      "building tree 53 of 548\n",
      "building tree 54 of 548\n",
      "building tree 55 of 548\n",
      "building tree 56 of 548\n",
      "building tree 57 of 548\n",
      "building tree 58 of 548\n",
      "building tree 59 of 548\n",
      "building tree 60 of 548\n",
      "building tree 61 of 548\n",
      "building tree 62 of 548\n",
      "building tree 63 of 548\n",
      "building tree 64 of 548\n",
      "building tree 65 of 548\n",
      "building tree 66 of 548\n",
      "building tree 67 of 548\n",
      "building tree 68 of 548\n",
      "building tree 69 of 548\n",
      "building tree 70 of 548\n",
      "building tree 71 of 548\n",
      "building tree 72 of 548\n",
      "building tree 73 of 548\n",
      "building tree 74 of 548\n",
      "building tree 75 of 548\n",
      "building tree 76 of 548\n",
      "building tree 77 of 548\n",
      "building tree 78 of 548\n",
      "building tree 79 of 548\n",
      "building tree 80 of 548\n",
      "building tree 81 of 548\n",
      "building tree 82 of 548\n",
      "building tree 83 of 548\n",
      "building tree 84 of 548\n",
      "building tree 85 of 548\n",
      "building tree 86 of 548\n",
      "building tree 87 of 548\n",
      "building tree 88 of 548\n",
      "building tree 89 of 548\n",
      "building tree 90 of 548\n",
      "building tree 91 of 548\n",
      "building tree 92 of 548\n",
      "building tree 93 of 548\n",
      "building tree 94 of 548\n",
      "building tree 95 of 548\n",
      "building tree 96 of 548\n",
      "building tree 97 of 548\n",
      "building tree 98 of 548\n",
      "building tree 99 of 548\n",
      "building tree 100 of 548\n",
      "building tree 101 of 548\n",
      "building tree 102 of 548\n",
      "building tree 103 of 548\n",
      "building tree 104 of 548\n",
      "building tree 105 of 548\n",
      "building tree 106 of 548\n",
      "building tree 107 of 548\n",
      "building tree 108 of 548\n",
      "building tree 109 of 548\n",
      "building tree 110 of 548\n",
      "building tree 111 of 548\n",
      "building tree 112 of 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   37.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 113 of 548\n",
      "building tree 114 of 548\n",
      "building tree 115 of 548\n",
      "building tree 116 of 548\n",
      "building tree 117 of 548\n",
      "building tree 118 of 548\n",
      "building tree 119 of 548\n",
      "building tree 120 of 548\n",
      "building tree 121 of 548\n",
      "building tree 122 of 548\n",
      "building tree 123 of 548\n",
      "building tree 124 of 548\n",
      "building tree 125 of 548\n",
      "building tree 126 of 548\n",
      "building tree 127 of 548\n",
      "building tree 128 of 548\n",
      "building tree 129 of 548\n",
      "building tree 130 of 548\n",
      "building tree 131 of 548\n",
      "building tree 132 of 548\n",
      "building tree 133 of 548\n",
      "building tree 134 of 548\n",
      "building tree 135 of 548\n",
      "building tree 136 of 548\n",
      "building tree 137 of 548\n",
      "building tree 138 of 548\n",
      "building tree 139 of 548\n",
      "building tree 140 of 548\n",
      "building tree 141 of 548\n",
      "building tree 142 of 548\n",
      "building tree 143 of 548\n",
      "building tree 144 of 548\n",
      "building tree 145 of 548\n",
      "building tree 146 of 548building tree 147 of 548\n",
      "building tree 148 of 548\n",
      "\n",
      "building tree 149 of 548\n",
      "building tree 150 of 548\n",
      "building tree 151 of 548\n",
      "building tree 152 of 548\n",
      "building tree 153 of 548\n",
      "building tree 154 of 548\n",
      "building tree 155 of 548\n",
      "building tree 156 of 548building tree 157 of 548\n",
      "\n",
      "building tree 158 of 548\n",
      "building tree 159 of 548\n",
      "building tree 160 of 548\n",
      "building tree 161 of 548\n",
      "building tree 162 of 548\n",
      "building tree 163 of 548\n",
      "building tree 164 of 548\n",
      "building tree 165 of 548\n",
      "building tree 166 of 548\n",
      "building tree 167 of 548\n",
      "building tree 168 of 548\n",
      "building tree 169 of 548\n",
      "building tree 170 of 548\n",
      "building tree 171 of 548\n",
      "building tree 172 of 548\n",
      "building tree 173 of 548\n",
      "building tree 174 of 548\n",
      "building tree 175 of 548\n",
      "building tree 176 of 548\n",
      "building tree 177 of 548\n",
      "building tree 178 of 548\n",
      "building tree 179 of 548\n",
      "building tree 180 of 548\n",
      "building tree 181 of 548\n",
      "building tree 182 of 548\n",
      "building tree 183 of 548\n",
      "building tree 184 of 548\n",
      "building tree 185 of 548\n",
      "building tree 186 of 548\n",
      "building tree 187 of 548\n",
      "building tree 188 of 548\n",
      "building tree 189 of 548\n",
      "building tree 190 of 548\n",
      "building tree 191 of 548\n",
      "building tree 192 of 548\n",
      "building tree 193 of 548\n",
      "building tree 194 of 548\n",
      "building tree 195 of 548\n",
      "building tree 196 of 548\n",
      "building tree 197 of 548\n",
      "building tree 198 of 548\n",
      "building tree 199 of 548\n",
      "building tree 200 of 548\n",
      "building tree 201 of 548\n",
      "building tree 202 of 548\n",
      "building tree 203 of 548\n",
      "building tree 204 of 548\n",
      "building tree 205 of 548\n",
      "building tree 206 of 548\n",
      "building tree 207 of 548\n",
      "building tree 208 of 548\n",
      "building tree 209 of 548\n",
      "building tree 210 of 548\n",
      "building tree 211 of 548\n",
      "building tree 212 of 548\n",
      "building tree 213 of 548\n",
      "building tree 214 of 548\n",
      "building tree 215 of 548\n",
      "building tree 216 of 548\n",
      "building tree 217 of 548\n",
      "building tree 218 of 548\n",
      "building tree 219 of 548\n",
      "building tree 220 of 548\n",
      "building tree 221 of 548\n",
      "building tree 222 of 548\n",
      "building tree 223 of 548\n",
      "building tree 224 of 548\n",
      "building tree 225 of 548\n",
      "building tree 226 of 548\n",
      "building tree 227 of 548\n",
      "building tree 228 of 548\n",
      "building tree 229 of 548\n",
      "building tree 230 of 548\n",
      "building tree 231 of 548\n",
      "building tree 232 of 548\n",
      "building tree 233 of 548\n",
      "building tree 234 of 548\n",
      "building tree 235 of 548\n",
      "building tree 236 of 548\n",
      "building tree 237 of 548\n",
      "building tree 238 of 548\n",
      "building tree 239 of 548\n",
      "building tree 240 of 548\n",
      "building tree 241 of 548\n",
      "building tree 242 of 548\n",
      "building tree 243 of 548\n",
      "building tree 244 of 548\n",
      "building tree 245 of 548\n",
      "building tree 246 of 548\n",
      "building tree 247 of 548\n",
      "building tree 248 of 548\n",
      "building tree 249 of 548\n",
      "building tree 250 of 548\n",
      "building tree 251 of 548\n",
      "building tree 252 of 548\n",
      "building tree 253 of 548\n",
      "building tree 254 of 548\n",
      "building tree 255 of 548\n",
      "building tree 256 of 548\n",
      "building tree 257 of 548\n",
      "building tree 258 of 548\n",
      "building tree 259 of 548\n",
      "building tree 260 of 548\n",
      "building tree 261 of 548\n",
      "building tree 262 of 548\n",
      "building tree 263 of 548\n",
      "building tree 264 of 548\n",
      "building tree 265 of 548\n",
      "building tree 266 of 548\n",
      "building tree 267 of 548\n",
      "building tree 268 of 548\n",
      "building tree 269 of 548\n",
      "building tree 270 of 548\n",
      "building tree 271 of 548\n",
      "building tree 272 of 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 273 of 548\n",
      "building tree 274 of 548\n",
      "building tree 275 of 548\n",
      "building tree 276 of 548\n",
      "building tree 277 of 548\n",
      "building tree 278 of 548\n",
      "building tree 279 of 548\n",
      "building tree 280 of 548\n",
      "building tree 281 of 548\n",
      "building tree 282 of 548\n",
      "building tree 283 of 548\n",
      "building tree 284 of 548\n",
      "building tree 285 of 548\n",
      "building tree 286 of 548\n",
      "building tree 287 of 548\n",
      "building tree 288 of 548\n",
      "building tree 289 of 548\n",
      "building tree 290 of 548\n",
      "building tree 291 of 548\n",
      "building tree 292 of 548\n",
      "building tree 293 of 548\n",
      "building tree 294 of 548\n",
      "building tree 295 of 548building tree 296 of 548\n",
      "\n",
      "building tree 297 of 548\n",
      "building tree 298 of 548\n",
      "building tree 299 of 548\n",
      "building tree 300 of 548\n",
      "building tree 301 of 548\n",
      "building tree 302 of 548\n",
      "building tree 303 of 548\n",
      "building tree 304 of 548\n",
      "building tree 305 of 548\n",
      "building tree 306 of 548\n",
      "building tree 307 of 548\n",
      "building tree 308 of 548\n",
      "building tree 309 of 548\n",
      "building tree 310 of 548\n",
      "building tree 311 of 548\n",
      "building tree 312 of 548\n",
      "building tree 313 of 548\n",
      "building tree 314 of 548\n",
      "building tree 315 of 548\n",
      "building tree 316 of 548\n",
      "building tree 317 of 548\n",
      "building tree 318 of 548\n",
      "building tree 319 of 548\n",
      "building tree 320 of 548\n",
      "building tree 321 of 548\n",
      "building tree 322 of 548\n",
      "building tree 323 of 548\n",
      "building tree 324 of 548\n",
      "building tree 325 of 548\n",
      "building tree 326 of 548\n",
      "building tree 327 of 548\n",
      "building tree 328 of 548\n",
      "building tree 329 of 548\n",
      "building tree 330 of 548\n",
      "building tree 331 of 548\n",
      "building tree 332 of 548\n",
      "building tree 333 of 548\n",
      "building tree 334 of 548\n",
      "building tree 335 of 548\n",
      "building tree 336 of 548\n",
      "building tree 337 of 548\n",
      "building tree 338 of 548\n",
      "building tree 339 of 548\n",
      "building tree 340 of 548\n",
      "building tree 341 of 548\n",
      "building tree 342 of 548\n",
      "building tree 343 of 548\n",
      "building tree 344 of 548\n",
      "building tree 345 of 548\n",
      "building tree 346 of 548\n",
      "building tree 347 of 548\n",
      "building tree 348 of 548\n",
      "building tree 349 of 548\n",
      "building tree 350 of 548\n",
      "building tree 351 of 548\n",
      "building tree 352 of 548\n",
      "building tree 353 of 548\n",
      "building tree 354 of 548\n",
      "building tree 355 of 548\n",
      "building tree 356 of 548\n",
      "building tree 357 of 548\n",
      "building tree 358 of 548\n",
      "building tree 359 of 548\n",
      "building tree 360 of 548\n",
      "building tree 361 of 548\n",
      "building tree 362 of 548\n",
      "building tree 363 of 548\n",
      "building tree 364 of 548\n",
      "building tree 365 of 548\n",
      "building tree 366 of 548\n",
      "building tree 367 of 548\n",
      "building tree 368 of 548\n",
      "building tree 369 of 548\n",
      "building tree 370 of 548\n",
      "building tree 371 of 548\n",
      "building tree 372 of 548\n",
      "building tree 373 of 548\n",
      "building tree 374 of 548\n",
      "building tree 375 of 548\n",
      "building tree 376 of 548\n",
      "building tree 377 of 548\n",
      "building tree 378 of 548\n",
      "building tree 379 of 548\n",
      "building tree 380 of 548\n",
      "building tree 381 of 548\n",
      "building tree 382 of 548\n",
      "building tree 383 of 548\n",
      "building tree 384 of 548\n",
      "building tree 385 of 548\n",
      "building tree 386 of 548\n",
      "building tree 387 of 548\n",
      "building tree 388 of 548\n",
      "building tree 389 of 548\n",
      "building tree 390 of 548\n",
      "building tree 391 of 548\n",
      "building tree 392 of 548\n",
      "building tree 393 of 548\n",
      "building tree 394 of 548\n",
      "building tree 395 of 548\n",
      "building tree 396 of 548\n",
      "building tree 397 of 548\n",
      "building tree 398 of 548\n",
      "building tree 399 of 548\n",
      "building tree 400 of 548\n",
      "building tree 401 of 548\n",
      "building tree 402 of 548\n",
      "building tree 403 of 548\n",
      "building tree 404 of 548\n",
      "building tree 405 of 548\n",
      "building tree 406 of 548\n",
      "building tree 407 of 548\n",
      "building tree 408 of 548\n",
      "building tree 409 of 548\n",
      "building tree 410 of 548\n",
      "building tree 411 of 548\n",
      "building tree 412 of 548\n",
      "building tree 413 of 548\n",
      "building tree 414 of 548\n",
      "building tree 415 of 548\n",
      "building tree 416 of 548\n",
      "building tree 417 of 548\n",
      "building tree 418 of 548\n",
      "building tree 419 of 548\n",
      "building tree 420 of 548\n",
      "building tree 421 of 548\n",
      "building tree 422 of 548\n",
      "building tree 423 of 548\n",
      "building tree 424 of 548\n",
      "building tree 425 of 548\n",
      "building tree 426 of 548\n",
      "building tree 427 of 548\n",
      "building tree 428 of 548\n",
      "building tree 429 of 548\n",
      "building tree 430 of 548\n",
      "building tree 431 of 548\n",
      "building tree 432 of 548\n",
      "building tree 433 of 548\n",
      "building tree 434 of 548\n",
      "building tree 435 of 548\n",
      "building tree 436 of 548\n",
      "building tree 437 of 548\n",
      "building tree 438 of 548\n",
      "building tree 439 of 548\n",
      "building tree 440 of 548\n",
      "building tree 441 of 548\n",
      "building tree 442 of 548\n",
      "building tree 443 of 548\n",
      "building tree 444 of 548\n",
      "building tree 445 of 548\n",
      "building tree 446 of 548\n",
      "building tree 447 of 548\n",
      "building tree 448 of 548\n",
      "building tree 449 of 548\n",
      "building tree 450 of 548\n",
      "building tree 451 of 548\n",
      "building tree 452 of 548\n",
      "building tree 453 of 548\n",
      "building tree 454 of 548\n",
      "building tree 455 of 548\n",
      "building tree 456 of 548\n",
      "building tree 457 of 548\n",
      "building tree 458 of 548\n",
      "building tree 459 of 548\n",
      "building tree 460 of 548\n",
      "building tree 461 of 548\n",
      "building tree 462 of 548\n",
      "building tree 463 of 548\n",
      "building tree 464 of 548\n",
      "building tree 465 of 548\n",
      "building tree 466 of 548\n",
      "building tree 467 of 548\n",
      "building tree 468 of 548\n",
      "building tree 469 of 548\n",
      "building tree 470 of 548\n",
      "building tree 471 of 548\n",
      "building tree 472 of 548\n",
      "building tree 473 of 548\n",
      "building tree 474 of 548\n",
      "building tree 475 of 548\n",
      "building tree 476 of 548\n",
      "building tree 477 of 548\n",
      "building tree 478 of 548\n",
      "building tree 479 of 548\n",
      "building tree 480 of 548\n",
      "building tree 481 of 548\n",
      "building tree 482 of 548\n",
      "building tree 483 of 548\n",
      "building tree 484 of 548\n",
      "building tree 485 of 548\n",
      "building tree 486 of 548\n",
      "building tree 487 of 548\n",
      "building tree 488 of 548\n",
      "building tree 489 of 548\n",
      "building tree 490 of 548\n",
      "building tree 491 of 548\n",
      "building tree 492 of 548\n",
      "building tree 493 of 548\n",
      "building tree 494 of 548\n",
      "building tree 495 of 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 496 of 548\n",
      "building tree 497 of 548\n",
      "building tree 498 of 548\n",
      "building tree 499 of 548\n",
      "building tree 500 of 548\n",
      "building tree 501 of 548\n",
      "building tree 502 of 548\n",
      "building tree 503 of 548\n",
      "building tree 504 of 548\n",
      "building tree 505 of 548\n",
      "building tree 506 of 548\n",
      "building tree 507 of 548\n",
      "building tree 508 of 548\n",
      "building tree 509 of 548\n",
      "building tree 510 of 548\n",
      "building tree 511 of 548\n",
      "building tree 512 of 548\n",
      "building tree 513 of 548\n",
      "building tree 514 of 548\n",
      "building tree 515 of 548\n",
      "building tree 516 of 548\n",
      "building tree 517 of 548\n",
      "building tree 518 of 548\n",
      "building tree 519 of 548\n",
      "building tree 520 of 548\n",
      "building tree 521 of 548\n",
      "building tree 522 of 548\n",
      "building tree 523 of 548\n",
      "building tree 524 of 548\n",
      "building tree 525 of 548\n",
      "building tree 526 of 548\n",
      "building tree 527 of 548\n",
      "building tree 528 of 548\n",
      "building tree 529 of 548\n",
      "building tree 530 of 548\n",
      "building tree 531 of 548\n",
      "building tree 532 of 548\n",
      "building tree 533 of 548\n",
      "building tree 534 of 548\n",
      "building tree 535 of 548\n",
      "building tree 536 of 548\n",
      "building tree 537 of 548\n",
      "building tree 538 of 548\n",
      "building tree 539 of 548\n",
      "building tree 540 of 548\n",
      "building tree 541 of 548\n",
      "building tree 542 of 548\n",
      "building tree 543 of 548\n",
      "building tree 544 of 548\n",
      "building tree 545 of 548\n",
      "building tree 546 of 548\n",
      "building tree 547 of 548\n",
      "building tree 548 of 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 548 out of 548 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>548</td>\n",
       "      <td>0.578004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>414</td>\n",
       "      <td>0.563029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>388</td>\n",
       "      <td>0.559961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>302</td>\n",
       "      <td>0.545599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>306</td>\n",
       "      <td>0.539707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  n_estimators     Score\n",
       "0         71           548  0.578004\n",
       "4        102           414  0.563029\n",
       "2         91           388  0.559961\n",
       "3         40           302  0.545599\n",
       "1         34           306  0.539707"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search = RandomForestClassifier(n_jobs=-1, verbose=3)\n",
    "random_search_rf = RandomizedSearchCV(rnd_search, parameter_grid, cv=KFold(n_splits=3, shuffle=False), random_state=42, verbose=3, n_iter=5)\n",
    "\n",
    "random_search_rf.fit(X_train_scaled,y_train)\n",
    "\n",
    "result = pd.concat([pd.DataFrame(random_search_rf.cv_results_[\"params\"]),pd.DataFrame(random_search_rf.cv_results_[\"mean_test_score\"], columns=[\"Score\"])],axis=1)\n",
    "result.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score is around 0.57. So RF can overfit well, but struggling to generalise. Would need more estimators for a better score but it doesn't look to be as promising as the NN. Why? I thought RF should be comparable given it is tabular data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test implementation ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only testing on one validation set is not robust right? You could do some sort of cross validation where you select different validation folds, but then training is just longer. I guess it depends on the size of the validation fold. But what is done in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_valid, X_train = X_val_scaled, X_train_scaled\n",
    "y_valid, y_train = y_val, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8147 samples, validate on 2037 samples\n",
      "Epoch 1/100\n",
      "8147/8147 [==============================] - 16s 2ms/sample - loss: 2.5306 - accuracy: 0.4026 - val_loss: 1.6465 - val_accuracy: 0.6122\n",
      "Epoch 2/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.8607 - accuracy: 0.8171 - val_loss: 1.1563 - val_accuracy: 0.7177\n",
      "Epoch 3/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.3593 - accuracy: 0.9440 - val_loss: 0.9329 - val_accuracy: 0.7658\n",
      "Epoch 4/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.1682 - accuracy: 0.9872 - val_loss: 0.8491 - val_accuracy: 0.7865\n",
      "Epoch 5/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0916 - accuracy: 0.9985 - val_loss: 0.7840 - val_accuracy: 0.7982\n",
      "Epoch 6/100\n",
      "8147/8147 [==============================] - 12s 1ms/sample - loss: 0.0590 - accuracy: 0.9999 - val_loss: 0.7654 - val_accuracy: 0.7963\n",
      "Epoch 7/100\n",
      "8147/8147 [==============================] - 12s 1ms/sample - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.7366 - val_accuracy: 0.8085\n",
      "Epoch 8/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.7318 - val_accuracy: 0.8051\n",
      "Epoch 9/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.7103 - val_accuracy: 0.8125\n",
      "Epoch 10/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.7058 - val_accuracy: 0.8081\n",
      "Epoch 11/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.6976 - val_accuracy: 0.8125\n",
      "Epoch 12/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.6927 - val_accuracy: 0.8144\n",
      "Epoch 13/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.8149\n",
      "Epoch 14/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.6808 - val_accuracy: 0.8159\n",
      "Epoch 15/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.8193\n",
      "Epoch 16/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.6743 - val_accuracy: 0.8189\n",
      "Epoch 17/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.8208\n",
      "Epoch 18/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 0.8198\n",
      "Epoch 19/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.6642 - val_accuracy: 0.8213\n",
      "Epoch 20/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.6595 - val_accuracy: 0.8228\n",
      "Epoch 21/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.6595 - val_accuracy: 0.8223\n",
      "Epoch 22/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.8223\n",
      "Epoch 23/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 0.8228\n",
      "Epoch 24/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 0.8247\n",
      "Epoch 25/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6497 - val_accuracy: 0.8272\n",
      "Epoch 26/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.8243\n",
      "Epoch 27/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.8243\n",
      "Epoch 28/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.8267\n",
      "Epoch 29/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.6443 - val_accuracy: 0.8277\n",
      "Epoch 30/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 0.8257\n",
      "Epoch 31/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.6419 - val_accuracy: 0.8272\n",
      "Epoch 32/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 0.8277\n",
      "Epoch 33/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.6390 - val_accuracy: 0.8267\n",
      "Epoch 34/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.8297\n",
      "Epoch 35/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 0.8292\n",
      "Epoch 36/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6359 - val_accuracy: 0.8297\n",
      "Epoch 37/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 0.8306\n",
      "Epoch 38/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.8301\n",
      "Epoch 39/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.6322 - val_accuracy: 0.8306\n",
      "Epoch 40/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 0.8301\n",
      "Epoch 41/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6322 - val_accuracy: 0.8306\n",
      "Epoch 42/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6316 - val_accuracy: 0.8306\n",
      "Epoch 43/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.6298 - val_accuracy: 0.8306\n",
      "Epoch 44/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6298 - val_accuracy: 0.8297\n",
      "Epoch 45/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6293 - val_accuracy: 0.8311\n",
      "Epoch 46/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 0.8306\n",
      "Epoch 47/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 0.8306\n",
      "Epoch 48/100\n",
      "8147/8147 [==============================] - 8s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 0.8311\n",
      "Epoch 49/100\n",
      "8147/8147 [==============================] - 6s 744us/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6257 - val_accuracy: 0.8316\n",
      "Epoch 50/100\n",
      "8147/8147 [==============================] - 6s 739us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.8316\n",
      "Epoch 51/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 0.8316\n",
      "Epoch 52/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6251 - val_accuracy: 0.8326\n",
      "Epoch 53/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6245 - val_accuracy: 0.8316\n",
      "Epoch 54/100\n",
      "8147/8147 [==============================] - 12s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 0.8321\n",
      "Epoch 55/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 0.8331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6221 - val_accuracy: 0.8321\n",
      "Epoch 57/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 0.8321\n",
      "Epoch 58/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 0.8331\n",
      "Epoch 59/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6214 - val_accuracy: 0.8326\n",
      "Epoch 60/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 0.8331\n",
      "Epoch 61/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.8336\n",
      "Epoch 62/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.8331\n",
      "Epoch 63/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 0.8341\n",
      "Epoch 64/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 0.8326\n",
      "Epoch 65/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.8326\n",
      "Epoch 66/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 0.8331\n",
      "Epoch 67/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.8336\n",
      "Epoch 68/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 0.8336\n",
      "Epoch 69/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6178 - val_accuracy: 0.8336\n",
      "Epoch 70/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.8336\n",
      "Epoch 71/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6168 - val_accuracy: 0.8331\n",
      "Epoch 72/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6165 - val_accuracy: 0.8331\n",
      "Epoch 73/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6166 - val_accuracy: 0.8336\n",
      "Epoch 74/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 0.8336\n",
      "Epoch 75/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.8341\n",
      "Epoch 76/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6159 - val_accuracy: 0.8341\n",
      "Epoch 77/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 0.8336\n",
      "Epoch 78/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 0.8336\n",
      "Epoch 79/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 0.8336\n",
      "Epoch 80/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.8336\n",
      "Epoch 81/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 0.8341\n",
      "Epoch 82/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.8341\n",
      "Epoch 83/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6138 - val_accuracy: 0.8341\n",
      "Epoch 84/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 0.8341\n",
      "Epoch 85/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 0.8341\n",
      "Epoch 86/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.8341\n",
      "Epoch 87/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 0.8341\n",
      "Epoch 88/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.8336\n",
      "Epoch 89/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.8336\n",
      "Epoch 90/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 0.8341\n",
      "Epoch 91/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 0.8341\n",
      "Epoch 92/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 0.8341\n",
      "Epoch 93/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6116 - val_accuracy: 0.8341\n",
      "Epoch 94/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 0.8341\n",
      "Epoch 95/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 0.8341\n",
      "Epoch 96/100\n",
      "8147/8147 [==============================] - 10s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 0.8341\n",
      "Epoch 97/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 0.8341\n",
      "Epoch 98/100\n",
      "8147/8147 [==============================] - 9s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 0.8341\n",
      "Epoch 99/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6106 - val_accuracy: 0.8341\n",
      "Epoch 100/100\n",
      "8147/8147 [==============================] - 11s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 0.8341\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(1000, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(500, activation=\"relu\"),\n",
    "    keras.layers.Dense(number_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "\n",
    "#callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5017/5017 [==============================] - 3s 567us/sample - loss: 1.3500 - accuracy: 0.7172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3499739765051662, 0.71716166]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sample##\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "\n",
    "\n",
    "tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_fn(y_train[:1], predictions).numpy()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
